{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0300c8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9cb0f",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "\n",
    "on a fpga a normal pytorch or tensorflow model wont work until it is converted to a format that the fpga can understand\n",
    "this is done using a tool called Vitis AI which is provided by Xilinx\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" \n",
    "to know about hls \n",
    "\n",
    "what is hls?\n",
    "\n",
    "HLS stands for High-Level Synthesis. It is a process that takes a high-level description of a design (usually in a language like C, C++, or SystemC) and converts it into a hardware description language (HDL) like VHDL or Verilog. This allows designers to work at a higher level of abstraction, making it easier to design complex systems without having to manually code in HDL.\n",
    "\n",
    "HLS is particularly useful for FPGA design because it can significantly reduce the time and effort required to create and optimize hardware implementations. By using HLS tools, designers can focus on the algorithm and functionality of their design, while the tool takes care of the low-level details of the hardware implementation.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "change of plans \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926d9307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 12:46:07.267665: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Flatten , Conv2D \n",
    "from keras.models import Sequential\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4feebbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['archive(1)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d02851",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"archive(1)/real_and_fake_face\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "735b0fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "y=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb657890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]4;0;#0E1C21\u001b\\\u001b]1;0;#0E1C21\u001b\\\u001b]4;1;#8383FF\u001b\\\u001b]4;2;#64DEE1\u001b\\\u001b]4;3;#75FCDD\u001b\\\u001b]4;4;#79B3C4\u001b\\\u001b]4;5;#7AAEEA\u001b\\\u001b]4;6;#81D7E1\u001b\\\u001b]4;7;#CCDBD5\u001b\\\u001b]4;8;#B1BCB5\u001b\\\u001b]4;9;#BCB9FF\u001b\\\u001b]4;10;#F2FFFF\u001b\\\u001b]4;11;#FFFFFF\u001b\\\u001b]4;12;#BFE1EA\u001b\\\u001b]4;13;#C8DAFF\u001b\\\u001b]4;14;#EDFDFF\u001b\\\u001b]4;15;#B7EAFF\u001b\\\u001b]10;#CCDBD5\u001b\\\u001b]11;[100]#0E1C21\u001b\\\u001b]12;#CCDBD5\u001b\\\u001b]13;#CCDBD5\u001b\\\u001b]17;#CCDBD5\u001b\\\u001b]19;#0E1C21\u001b\\\u001b]4;232;#CCDBD5\u001b\\\u001b]4;256;#CCDBD5\u001b\\\u001b]708;[100]#0E1C21\u001b\\\u001b]11;#0E1C21\u001b\\\n",
      "\u001b[38;5;4m\u001b[1m real_and_fake_face\u001b[0m  \u001b[38;5;4m\u001b[1m real_and_fake_face_detection\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ls archive\\(1\\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e35fc415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "base_path=\"archive/real_and_fake_face\"\n",
    "X=[]\n",
    "y=[]\n",
    "train_real_dir=os.path.join(base_path,\"training_real\")\n",
    "train_fake_dir=os.path.join(base_path,\"training_fake\")\n",
    "\n",
    "IMG_SIZE=(128,128)\n",
    "\n",
    "for filename in os.listdir(train_real_dir):\n",
    "    img_path=os.path.join(train_real_dir,filename)\n",
    "    img = cv2.imread(img_path) \n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,IMG_SIZE)\n",
    "        X.append(img)\n",
    "        y.append(1)\n",
    "for filename in os.listdir(train_fake_dir):\n",
    "    img_path=os.path.join(train_fake_dir,filename)\n",
    "    img = cv2.imread(img_path) \n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,IMG_SIZE)\n",
    "        X.append(img)\n",
    "        y.append(0)\n",
    "\n",
    "X=np.array(X)\n",
    "y=np.array(y)\n",
    "X=X/255.0\n",
    "\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "\n",
    "base_path=\"archive/real_and_fake_face_detection\"\n",
    "test_real_dir=os.path.join(base_path,\"training_real\")\n",
    "test_fake_dir=os.path.join(base_path,\"training_fake\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "032a6c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"/home/waasi/waasi/python/kura_fpga/archive/real_and_fake_face_detection/real_and_fake_face\"\n",
    "test_real_dir=os.path.join(base_path,\"training_real\")\n",
    "test_fake_dir=os.path.join(base_path,\"training_fake\")\n",
    "IMG_SIZE=(128,128)\n",
    "for filename in os.listdir(test_real_dir):\n",
    "    img_path=os.path.join(test_real_dir,filename)\n",
    "    img = cv2.imread(img_path) \n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,IMG_SIZE)\n",
    "        x_test.append(img)\n",
    "        y_test.append(1)\n",
    "for filename in os.listdir(test_fake_dir):\n",
    "    img_path=os.path.join(test_fake_dir,filename)\n",
    "    img = cv2.imread(img_path) \n",
    "    if img is not None:\n",
    "        img = cv2.resize(img,IMG_SIZE)\n",
    "        x_test.append(img)\n",
    "        y_test.append(0)\n",
    "\n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "x_test=x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a2d4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "x_train_tensor = torch.tensor(X, dtype=torch.float32).permute(0, 3, 1, 2)  # Change to (N, C, H, W)\n",
    "y_train_tensor = torch.tensor(y, dtype=torch.long)  \n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32).permute(0, 3, 1, 2)  # Change to (N, C, H, W)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb084506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjusted for 128x128 input size\n",
    "        self.fc2 = nn.Linear(128, 2)  # Binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)  # Flatten the tensor\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88ed2774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7614622209221125\n",
      "Epoch 2, Loss: 0.6852169428020716\n",
      "Epoch 3, Loss: 0.6728378226980567\n",
      "Epoch 4, Loss: 0.6680823666974902\n",
      "Epoch 5, Loss: 0.6588665349408984\n",
      "Epoch 6, Loss: 0.6463717725127935\n",
      "Epoch 7, Loss: 0.6287095723673701\n",
      "Epoch 8, Loss: 0.6169989723712206\n",
      "Epoch 9, Loss: 0.5778119997121394\n",
      "Epoch 10, Loss: 0.5471758833155036\n"
     ]
    }
   ],
   "source": [
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2cb479f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model accuracy\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2becd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.01224889759922%\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy\n",
    "print(f\"Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c594f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do i save the model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c619a569",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the model that i have saved\n",
    "model.load_state_dict(torch.load(\"model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f2d7a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "# i can now predict an external image\n",
    "img_path=\"IMG_0234.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img,IMG_SIZE)\n",
    "img = img/255.0\n",
    "img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Change to (1, C, H, W)\n",
    "output = model(img_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f\"Predicted class: {predicted.item()}\")  # 0 for fake, 1 for real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8f980ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "img_path=\"IMG_20240808_091916928_BURST000_COVER_TOP.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img,IMG_SIZE)\n",
    "img = img/255.0\n",
    "img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Change to (1, C, H, W)\n",
    "output = model(img_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f\"Predicted class: {predicted.item()}\")  # 0 for fake, 1 for realtem()}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b564522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 1\n"
     ]
    }
   ],
   "source": [
    "img_path=\"woman_with_fake_boobs.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img,IMG_SIZE)\n",
    "img = img/255.0\n",
    "img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0,1).unsqueeze(0)  # Change to (1, C, H, W)\n",
    "output = model(img_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f\"Predicted class: {predicted.item()}\")  # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf1d8237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 0\n"
     ]
    }
   ],
   "source": [
    "img_path=\"zoro.jpg\"\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.resize(img,IMG_SIZE)\n",
    "img = img/255.0\n",
    "img_tensor = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Change to (1, C, H, W)\n",
    "output = model(img_tensor)\n",
    "_, predicted = torch.max(output.data, 1)\n",
    "print(f\"Predicted class: {predicted.item()}\")  # 0 for fake, 1 for real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36249156",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
